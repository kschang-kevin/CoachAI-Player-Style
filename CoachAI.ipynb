{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "badminton_data = pd.read_csv('./dataset.csv')\n",
    "badminton_data = badminton_data[['rally', 'player', 'type', 'hit_height', 'landing_area', 'player_location_area','time_proportion']]\n",
    "le = LabelEncoder()\n",
    "badminton_data['type'] = le.fit_transform(badminton_data['type'])\n",
    "training_data = badminton_data[:10789].reset_index(drop=True)\n",
    "testing_data = badminton_data[10789:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BadmintonDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(BadmintonDataset, self).__init__()\n",
    "        rally = []\n",
    "        mem = data['rally'][0]\n",
    "        tmp = []\n",
    "        for i in range(len(data)):\n",
    "            if i == len(data)-1:\n",
    "                rally.append(tmp)\n",
    "            if data['rally'][i] != mem:\n",
    "                mem = data['rally'][i+1]\n",
    "                rally.append(tmp)\n",
    "                tmp = []\n",
    "            tmp.append([data['player'][i], data['type'][i], data['hit_height'][i], data['landing_area'][i], data['player_location_area'][i], data['time_proportion'][i]])\n",
    "\n",
    "        self.pattern = []\n",
    "        self.label = []\n",
    "        for i in range(len(rally)):\n",
    "            A = []\n",
    "            B = []\n",
    "            for j in range(len(rally[i])):\n",
    "                if rally[i][j][0] == 'CHOU Tien Chen':\n",
    "                    A.append(np.concatenate((np.array(rally[i][j][1]) * np.ones(1), np.array(rally[i][j][2]) * np.ones(1), np.array(rally[i][j][3]) * np.ones(1), np.array(rally[i][j][4]) * np.ones(1), np.array(rally[i][j][5]) * np.ones(1))))\n",
    "                else:\n",
    "                    B.append(np.concatenate((np.array(rally[i][j][1]) * np.ones(1), np.array(rally[i][j][2]) * np.ones(1), np.array(rally[i][j][3]) * np.ones(1), np.array(rally[i][j][4]) * np.ones(1), np.array(rally[i][j][5]) * np.ones(1))))\n",
    "            if len(A) == 0 or len(B) == 0:\n",
    "                continue\n",
    "            self.pattern.append(np.concatenate((np.array(A), np.full((50 - len(A), 5), -10))))\n",
    "            self.label.append(np.array([0]))\n",
    "            self.pattern.append(np.concatenate((np.array(B), np.full((50 - len(B), 5), -10))))\n",
    "            self.label.append(np.array([1]))\n",
    "        self.label = np.array(self.label)\n",
    "        self.pattern = np.array(self.pattern)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.pattern[index], self.label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = BadmintonDataset(badminton_data)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(250, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 16),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 250),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        codes = self.encoder(inputs)\n",
    "        decoded = self.decoder(codes)\n",
    "        return codes, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200] Loss: 92.96614837646484\n",
      "[2/200] Loss: 92.63709259033203\n",
      "[3/200] Loss: 92.15216064453125\n",
      "[4/200] Loss: 92.95531463623047\n",
      "[5/200] Loss: 91.78447723388672\n",
      "[6/200] Loss: 92.12538146972656\n",
      "[7/200] Loss: 92.52552032470703\n",
      "[8/200] Loss: 91.74050903320312\n",
      "[9/200] Loss: 92.91446685791016\n",
      "[10/200] Loss: 92.33477020263672\n",
      "[11/200] Loss: 91.59191131591797\n",
      "[12/200] Loss: 91.61124420166016\n",
      "[13/200] Loss: 92.20622253417969\n",
      "[14/200] Loss: 91.15541076660156\n",
      "[15/200] Loss: 92.34761810302734\n",
      "[16/200] Loss: 92.90929412841797\n",
      "[17/200] Loss: 93.52404022216797\n",
      "[18/200] Loss: 92.68351745605469\n",
      "[19/200] Loss: 92.12432098388672\n",
      "[20/200] Loss: 91.64353942871094\n",
      "[21/200] Loss: 92.20222473144531\n",
      "[22/200] Loss: 92.48206329345703\n",
      "[23/200] Loss: 92.02784729003906\n",
      "[24/200] Loss: 92.7489013671875\n",
      "[25/200] Loss: 92.60089874267578\n",
      "[26/200] Loss: 92.16314697265625\n",
      "[27/200] Loss: 92.05864715576172\n",
      "[28/200] Loss: 92.53560638427734\n",
      "[29/200] Loss: 92.39261627197266\n",
      "[30/200] Loss: 92.37794494628906\n",
      "[31/200] Loss: 92.29981994628906\n",
      "[32/200] Loss: 92.18049621582031\n",
      "[33/200] Loss: 93.73456573486328\n",
      "[34/200] Loss: 91.45429229736328\n",
      "[35/200] Loss: 93.70165252685547\n",
      "[36/200] Loss: 92.427978515625\n",
      "[37/200] Loss: 92.78169250488281\n",
      "[38/200] Loss: 91.9825439453125\n",
      "[39/200] Loss: 92.2269287109375\n",
      "[40/200] Loss: 92.68292999267578\n",
      "[41/200] Loss: 92.78759765625\n",
      "[42/200] Loss: 91.2801742553711\n",
      "[43/200] Loss: 92.4725112915039\n",
      "[44/200] Loss: 92.30081176757812\n",
      "[45/200] Loss: 93.18461608886719\n",
      "[46/200] Loss: 92.41204071044922\n",
      "[47/200] Loss: 92.0687255859375\n",
      "[48/200] Loss: 91.65149688720703\n",
      "[49/200] Loss: 93.03932189941406\n",
      "[50/200] Loss: 91.38506317138672\n",
      "[51/200] Loss: 92.45384216308594\n",
      "[52/200] Loss: 92.8960189819336\n",
      "[53/200] Loss: 92.5036849975586\n",
      "[54/200] Loss: 92.58660888671875\n",
      "[55/200] Loss: 93.14427185058594\n",
      "[56/200] Loss: 92.00292205810547\n",
      "[57/200] Loss: 92.06255340576172\n",
      "[58/200] Loss: 93.13095092773438\n",
      "[59/200] Loss: 92.3125\n",
      "[60/200] Loss: 93.00856018066406\n",
      "[61/200] Loss: 92.33679962158203\n",
      "[62/200] Loss: 92.17699432373047\n",
      "[63/200] Loss: 91.7531509399414\n",
      "[64/200] Loss: 92.60182189941406\n",
      "[65/200] Loss: 91.99437713623047\n",
      "[66/200] Loss: 91.76273345947266\n",
      "[67/200] Loss: 92.7175521850586\n",
      "[68/200] Loss: 93.22218322753906\n",
      "[69/200] Loss: 92.02125549316406\n",
      "[70/200] Loss: 92.43801879882812\n",
      "[71/200] Loss: 93.1279067993164\n",
      "[72/200] Loss: 92.13367462158203\n",
      "[73/200] Loss: 91.69139099121094\n",
      "[74/200] Loss: 91.57763671875\n",
      "[75/200] Loss: 91.61922454833984\n",
      "[76/200] Loss: 92.12960815429688\n",
      "[77/200] Loss: 92.77641296386719\n",
      "[78/200] Loss: 91.73564910888672\n",
      "[79/200] Loss: 92.9118423461914\n",
      "[80/200] Loss: 92.58740234375\n",
      "[81/200] Loss: 92.86930084228516\n",
      "[82/200] Loss: 92.86886596679688\n",
      "[83/200] Loss: 93.0135726928711\n",
      "[84/200] Loss: 92.12193298339844\n",
      "[85/200] Loss: 92.68254089355469\n",
      "[86/200] Loss: 91.81184387207031\n",
      "[87/200] Loss: 91.80329132080078\n",
      "[88/200] Loss: 92.19554138183594\n",
      "[89/200] Loss: 92.85200500488281\n",
      "[90/200] Loss: 92.8371353149414\n",
      "[91/200] Loss: 92.88236999511719\n",
      "[92/200] Loss: 92.72906494140625\n",
      "[93/200] Loss: 92.38494110107422\n",
      "[94/200] Loss: 92.39291381835938\n",
      "[95/200] Loss: 92.64425659179688\n",
      "[96/200] Loss: 92.60453796386719\n",
      "[97/200] Loss: 91.66171264648438\n",
      "[98/200] Loss: 91.9331283569336\n",
      "[99/200] Loss: 91.62728118896484\n",
      "[100/200] Loss: 92.21585845947266\n",
      "[101/200] Loss: 93.26091003417969\n",
      "[102/200] Loss: 91.90243530273438\n",
      "[103/200] Loss: 92.60469055175781\n",
      "[104/200] Loss: 93.19290924072266\n",
      "[105/200] Loss: 90.73882293701172\n",
      "[106/200] Loss: 92.65911865234375\n",
      "[107/200] Loss: 90.83415985107422\n",
      "[108/200] Loss: 92.93336486816406\n",
      "[109/200] Loss: 92.6736831665039\n",
      "[110/200] Loss: 92.50540161132812\n",
      "[111/200] Loss: 93.0322036743164\n",
      "[112/200] Loss: 92.70276641845703\n",
      "[113/200] Loss: 92.66399383544922\n",
      "[114/200] Loss: 92.92366790771484\n",
      "[115/200] Loss: 92.13166809082031\n",
      "[116/200] Loss: 91.26156616210938\n",
      "[117/200] Loss: 92.93495178222656\n",
      "[118/200] Loss: 92.84027862548828\n",
      "[119/200] Loss: 92.10856628417969\n",
      "[120/200] Loss: 93.07121276855469\n",
      "[121/200] Loss: 93.00906372070312\n",
      "[122/200] Loss: 92.9470443725586\n",
      "[123/200] Loss: 92.53571319580078\n",
      "[124/200] Loss: 92.89934539794922\n",
      "[125/200] Loss: 91.9452133178711\n",
      "[126/200] Loss: 92.58654022216797\n",
      "[127/200] Loss: 92.85649871826172\n",
      "[128/200] Loss: 92.56367492675781\n",
      "[129/200] Loss: 93.2996597290039\n",
      "[130/200] Loss: 92.43758392333984\n",
      "[131/200] Loss: 91.56803894042969\n",
      "[132/200] Loss: 92.86443328857422\n",
      "[133/200] Loss: 92.67471313476562\n",
      "[134/200] Loss: 92.16741180419922\n",
      "[135/200] Loss: 93.1607437133789\n",
      "[136/200] Loss: 92.40160369873047\n",
      "[137/200] Loss: 91.93221282958984\n",
      "[138/200] Loss: 92.00260925292969\n",
      "[139/200] Loss: 92.99613952636719\n",
      "[140/200] Loss: 91.8395004272461\n",
      "[141/200] Loss: 92.58421325683594\n",
      "[142/200] Loss: 91.7560806274414\n",
      "[143/200] Loss: 92.71924591064453\n",
      "[144/200] Loss: 91.44290924072266\n",
      "[145/200] Loss: 90.94940948486328\n",
      "[146/200] Loss: 92.73774719238281\n",
      "[147/200] Loss: 93.54874420166016\n",
      "[148/200] Loss: 92.64141082763672\n",
      "[149/200] Loss: 92.01395416259766\n",
      "[150/200] Loss: 92.30403900146484\n",
      "[151/200] Loss: 91.55382537841797\n",
      "[152/200] Loss: 92.62428283691406\n",
      "[153/200] Loss: 91.73507690429688\n",
      "[154/200] Loss: 92.77743530273438\n",
      "[155/200] Loss: 92.4971923828125\n",
      "[156/200] Loss: 93.0797348022461\n",
      "[157/200] Loss: 92.19718933105469\n",
      "[158/200] Loss: 91.18778991699219\n",
      "[159/200] Loss: 93.30956268310547\n",
      "[160/200] Loss: 92.56070709228516\n",
      "[161/200] Loss: 92.04888153076172\n",
      "[162/200] Loss: 91.30406188964844\n",
      "[163/200] Loss: 93.125\n",
      "[164/200] Loss: 91.62184143066406\n",
      "[165/200] Loss: 92.89801788330078\n",
      "[166/200] Loss: 92.57508087158203\n",
      "[167/200] Loss: 92.10298919677734\n",
      "[168/200] Loss: 92.25260925292969\n",
      "[169/200] Loss: 92.04362487792969\n",
      "[170/200] Loss: 92.30428314208984\n",
      "[171/200] Loss: 91.18911743164062\n",
      "[172/200] Loss: 92.62741088867188\n",
      "[173/200] Loss: 92.81393432617188\n",
      "[174/200] Loss: 91.77680206298828\n",
      "[175/200] Loss: 92.05045318603516\n",
      "[176/200] Loss: 92.70360565185547\n",
      "[177/200] Loss: 92.15544128417969\n",
      "[178/200] Loss: 92.33997344970703\n",
      "[179/200] Loss: 93.01165771484375\n",
      "[180/200] Loss: 92.58244323730469\n",
      "[181/200] Loss: 92.15763854980469\n",
      "[182/200] Loss: 92.78023529052734\n",
      "[183/200] Loss: 92.44091033935547\n",
      "[184/200] Loss: 92.47100067138672\n",
      "[185/200] Loss: 91.25858306884766\n",
      "[186/200] Loss: 91.85983276367188\n",
      "[187/200] Loss: 91.00431060791016\n",
      "[188/200] Loss: 93.31084442138672\n",
      "[189/200] Loss: 91.73326873779297\n",
      "[190/200] Loss: 91.97870635986328\n",
      "[191/200] Loss: 92.24476623535156\n",
      "[192/200] Loss: 91.81599426269531\n",
      "[193/200] Loss: 91.82760620117188\n",
      "[194/200] Loss: 92.45332336425781\n",
      "[195/200] Loss: 92.61558532714844\n",
      "[196/200] Loss: 92.79096221923828\n",
      "[197/200] Loss: 93.23528289794922\n",
      "[198/200] Loss: 92.39015197753906\n",
      "[199/200] Loss: 91.59513854980469\n",
      "[200/200] Loss: 92.34407043457031\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder_model = AutoEncoder()\n",
    "optimizer = torch.optim.Adam(autoencoder_model.parameters(), lr=LR)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "autoencoder_model.to(device)\n",
    "loss_function.to(device)\n",
    "\n",
    "# Train\n",
    "for epoch in range(EPOCHS):\n",
    "    for data, labels in train_loader:\n",
    "        inputs = data.view(-1, 250)\n",
    "        inputs = inputs.to(device).float()\n",
    "        # Forward\n",
    "        codes, decoded = autoencoder_model(inputs)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(decoded, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Show progress\n",
    "    print('[{}/{}] Loss:'.format(epoch+1, EPOCHS), loss.item())\n",
    "\n",
    "# Save\n",
    "torch.save(autoencoder_model, 'autoencoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=250, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=16, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=128, out_features=250, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_model = torch.load('autoencoder.pt')\n",
    "autoencoder_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, state_size=16):\n",
    "        super(FFN, self).__init__()\n",
    "\n",
    "        self.lr1 = nn.Linear(state_size, state_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lr2 = nn.Linear(state_size, state_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.pred = nn.Linear(state_size, 1)     \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.lr1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lr2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pred(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ffn_model = FFN(16)\n",
    "optimizer = torch.optim.Adam(ffn_model.parameters(), lr=LR)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "ffn_model.to(device)\n",
    "loss_function.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200] Loss: 0.6901876926422119\n",
      "[2/200] Loss: 0.6980188488960266\n",
      "[3/200] Loss: 0.6945430636405945\n",
      "[4/200] Loss: 0.6961071491241455\n",
      "[5/200] Loss: 0.6925288438796997\n",
      "[6/200] Loss: 0.6934583783149719\n",
      "[7/200] Loss: 0.695493221282959\n",
      "[8/200] Loss: 0.692719578742981\n",
      "[9/200] Loss: 0.6940052509307861\n",
      "[10/200] Loss: 0.6944000720977783\n",
      "[11/200] Loss: 0.6957583427429199\n",
      "[12/200] Loss: 0.6920022964477539\n",
      "[13/200] Loss: 0.6929763555526733\n",
      "[14/200] Loss: 0.6914073824882507\n",
      "[15/200] Loss: 0.693361222743988\n",
      "[16/200] Loss: 0.6927630305290222\n",
      "[17/200] Loss: 0.6958842873573303\n",
      "[18/200] Loss: 0.6930668354034424\n",
      "[19/200] Loss: 0.6938624978065491\n",
      "[20/200] Loss: 0.6934561729431152\n",
      "[21/200] Loss: 0.6935862302780151\n",
      "[22/200] Loss: 0.6928880214691162\n",
      "[23/200] Loss: 0.6940085887908936\n",
      "[24/200] Loss: 0.6938445568084717\n",
      "[25/200] Loss: 0.6923574209213257\n",
      "[26/200] Loss: 0.6941049695014954\n",
      "[27/200] Loss: 0.6932421922683716\n",
      "[28/200] Loss: 0.6923415064811707\n",
      "[29/200] Loss: 0.6933371424674988\n",
      "[30/200] Loss: 0.6934695839881897\n",
      "[31/200] Loss: 0.6939748525619507\n",
      "[32/200] Loss: 0.6931281685829163\n",
      "[33/200] Loss: 0.6921391487121582\n",
      "[34/200] Loss: 0.6931365728378296\n",
      "[35/200] Loss: 0.6928197145462036\n",
      "[36/200] Loss: 0.6935430765151978\n",
      "[37/200] Loss: 0.6925959587097168\n",
      "[38/200] Loss: 0.6932167410850525\n",
      "[39/200] Loss: 0.6940474510192871\n",
      "[40/200] Loss: 0.6930738687515259\n",
      "[41/200] Loss: 0.693371057510376\n",
      "[42/200] Loss: 0.6928814053535461\n",
      "[43/200] Loss: 0.6930109262466431\n",
      "[44/200] Loss: 0.6956996321678162\n",
      "[45/200] Loss: 0.6940001845359802\n",
      "[46/200] Loss: 0.693798840045929\n",
      "[47/200] Loss: 0.6928881406784058\n",
      "[48/200] Loss: 0.6937844753265381\n",
      "[49/200] Loss: 0.6931139826774597\n",
      "[50/200] Loss: 0.6930575966835022\n",
      "[51/200] Loss: 0.6931793093681335\n",
      "[52/200] Loss: 0.693333625793457\n",
      "[53/200] Loss: 0.6940454840660095\n",
      "[54/200] Loss: 0.6930729746818542\n",
      "[55/200] Loss: 0.693406343460083\n",
      "[56/200] Loss: 0.6931300759315491\n",
      "[57/200] Loss: 0.6931844353675842\n",
      "[58/200] Loss: 0.6974321603775024\n",
      "[59/200] Loss: 0.6930122375488281\n",
      "[60/200] Loss: 0.6932482123374939\n",
      "[61/200] Loss: 0.6932924389839172\n",
      "[62/200] Loss: 0.6931301951408386\n",
      "[63/200] Loss: 0.6932434439659119\n",
      "[64/200] Loss: 0.6931533813476562\n",
      "[65/200] Loss: 0.6931396126747131\n",
      "[66/200] Loss: 0.6932461261749268\n",
      "[67/200] Loss: 0.6928684711456299\n",
      "[68/200] Loss: 0.6934000253677368\n",
      "[69/200] Loss: 0.6932718753814697\n",
      "[70/200] Loss: 0.69329833984375\n",
      "[71/200] Loss: 0.6935896873474121\n",
      "[72/200] Loss: 0.6934465169906616\n",
      "[73/200] Loss: 0.6937782168388367\n",
      "[74/200] Loss: 0.6929949522018433\n",
      "[75/200] Loss: 0.6934045553207397\n",
      "[76/200] Loss: 0.6923384666442871\n",
      "[77/200] Loss: 0.6939222812652588\n",
      "[78/200] Loss: 0.6938950419425964\n",
      "[79/200] Loss: 0.6929129958152771\n",
      "[80/200] Loss: 0.6931774020195007\n",
      "[81/200] Loss: 0.6933478713035583\n",
      "[82/200] Loss: 0.6934424638748169\n",
      "[83/200] Loss: 0.6938221454620361\n",
      "[84/200] Loss: 0.6931291222572327\n",
      "[85/200] Loss: 0.6932275891304016\n",
      "[86/200] Loss: 0.6934299468994141\n",
      "[87/200] Loss: 0.6923854947090149\n",
      "[88/200] Loss: 0.6932003498077393\n",
      "[89/200] Loss: 0.6934423446655273\n",
      "[90/200] Loss: 0.6940034031867981\n",
      "[91/200] Loss: 0.6935209631919861\n",
      "[92/200] Loss: 0.6931407451629639\n",
      "[93/200] Loss: 0.6928690671920776\n",
      "[94/200] Loss: 0.6933363080024719\n",
      "[95/200] Loss: 0.6939220428466797\n",
      "[96/200] Loss: 0.6936467289924622\n",
      "[97/200] Loss: 0.6934729814529419\n",
      "[98/200] Loss: 0.6933897733688354\n",
      "[99/200] Loss: 0.6941792368888855\n",
      "[100/200] Loss: 0.6930009126663208\n",
      "[101/200] Loss: 0.6935980319976807\n",
      "[102/200] Loss: 0.6932932734489441\n",
      "[103/200] Loss: 0.6937124133110046\n",
      "[104/200] Loss: 0.6926906704902649\n",
      "[105/200] Loss: 0.694067656993866\n",
      "[106/200] Loss: 0.6939518451690674\n",
      "[107/200] Loss: 0.6941043138504028\n",
      "[108/200] Loss: 0.6931712031364441\n",
      "[109/200] Loss: 0.6935736536979675\n",
      "[110/200] Loss: 0.6934521198272705\n",
      "[111/200] Loss: 0.693046510219574\n",
      "[112/200] Loss: 0.6928995847702026\n",
      "[113/200] Loss: 0.6936650276184082\n",
      "[114/200] Loss: 0.6923297643661499\n",
      "[115/200] Loss: 0.6932996511459351\n",
      "[116/200] Loss: 0.6945257186889648\n",
      "[117/200] Loss: 0.6953500509262085\n",
      "[118/200] Loss: 0.693842887878418\n",
      "[119/200] Loss: 0.6936985850334167\n",
      "[120/200] Loss: 0.694709062576294\n",
      "[121/200] Loss: 0.693163275718689\n",
      "[122/200] Loss: 0.6930666565895081\n",
      "[123/200] Loss: 0.6939716339111328\n",
      "[124/200] Loss: 0.6930853128433228\n",
      "[125/200] Loss: 0.6925948858261108\n",
      "[126/200] Loss: 0.6941795349121094\n",
      "[127/200] Loss: 0.693843424320221\n",
      "[128/200] Loss: 0.6916886568069458\n",
      "[129/200] Loss: 0.6931864023208618\n",
      "[130/200] Loss: 0.6931729912757874\n",
      "[131/200] Loss: 0.693437397480011\n",
      "[132/200] Loss: 0.6934763789176941\n",
      "[133/200] Loss: 0.6928769946098328\n",
      "[134/200] Loss: 0.6934877634048462\n",
      "[135/200] Loss: 0.6935724020004272\n",
      "[136/200] Loss: 0.6937456130981445\n",
      "[137/200] Loss: 0.6929014325141907\n",
      "[138/200] Loss: 0.6942532062530518\n",
      "[139/200] Loss: 0.6931244730949402\n",
      "[140/200] Loss: 0.6932135820388794\n",
      "[141/200] Loss: 0.6927811503410339\n",
      "[142/200] Loss: 0.6933842897415161\n",
      "[143/200] Loss: 0.6922446489334106\n",
      "[144/200] Loss: 0.6934154033660889\n",
      "[145/200] Loss: 0.6933884620666504\n",
      "[146/200] Loss: 0.6931911110877991\n",
      "[147/200] Loss: 0.6932149529457092\n",
      "[148/200] Loss: 0.6931368112564087\n",
      "[149/200] Loss: 0.6932399868965149\n",
      "[150/200] Loss: 0.6931124925613403\n",
      "[151/200] Loss: 0.6937832832336426\n",
      "[152/200] Loss: 0.6933466792106628\n",
      "[153/200] Loss: 0.6943300366401672\n",
      "[154/200] Loss: 0.6931874752044678\n",
      "[155/200] Loss: 0.6942165493965149\n",
      "[156/200] Loss: 0.6940112113952637\n",
      "[157/200] Loss: 0.6943464279174805\n",
      "[158/200] Loss: 0.6935499310493469\n",
      "[159/200] Loss: 0.6932567358016968\n",
      "[160/200] Loss: 0.6946178674697876\n",
      "[161/200] Loss: 0.6930511593818665\n",
      "[162/200] Loss: 0.6935164332389832\n",
      "[163/200] Loss: 0.6939166784286499\n",
      "[164/200] Loss: 0.6950950026512146\n",
      "[165/200] Loss: 0.6933091282844543\n",
      "[166/200] Loss: 0.6933138370513916\n",
      "[167/200] Loss: 0.694939374923706\n",
      "[168/200] Loss: 0.6925095319747925\n",
      "[169/200] Loss: 0.6949100494384766\n",
      "[170/200] Loss: 0.6932958364486694\n",
      "[171/200] Loss: 0.692966639995575\n",
      "[172/200] Loss: 0.6936622262001038\n",
      "[173/200] Loss: 0.6928669214248657\n",
      "[174/200] Loss: 0.69293212890625\n",
      "[175/200] Loss: 0.6935263276100159\n",
      "[176/200] Loss: 0.6941283345222473\n",
      "[177/200] Loss: 0.6934164762496948\n",
      "[178/200] Loss: 0.6929898858070374\n",
      "[179/200] Loss: 0.6929816007614136\n",
      "[180/200] Loss: 0.6940135955810547\n",
      "[181/200] Loss: 0.6932705044746399\n",
      "[182/200] Loss: 0.6933190226554871\n",
      "[183/200] Loss: 0.6932275891304016\n",
      "[184/200] Loss: 0.693847119808197\n",
      "[185/200] Loss: 0.6931791305541992\n",
      "[186/200] Loss: 0.6930830478668213\n",
      "[187/200] Loss: 0.6931458711624146\n",
      "[188/200] Loss: 0.6931882500648499\n",
      "[189/200] Loss: 0.6937462687492371\n",
      "[190/200] Loss: 0.6935973763465881\n",
      "[191/200] Loss: 0.6934126019477844\n",
      "[192/200] Loss: 0.6936962008476257\n",
      "[193/200] Loss: 0.6930845379829407\n",
      "[194/200] Loss: 0.6937123537063599\n",
      "[195/200] Loss: 0.6939328908920288\n",
      "[196/200] Loss: 0.6933949589729309\n",
      "[197/200] Loss: 0.6931711435317993\n",
      "[198/200] Loss: 0.6932432055473328\n",
      "[199/200] Loss: 0.693054735660553\n",
      "[200/200] Loss: 0.6933213472366333\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for data, label in train_loader:\n",
    "        inputs = data.view(-1, 250)\n",
    "        inputs = inputs.to(device).float()\n",
    "        label = label.to(device).float()\n",
    "        # Forward\n",
    "        codes, decoded = autoencoder_model(inputs)\n",
    "        output = ffn_model(codes)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('[{}/{}] Loss:'.format(epoch+1, EPOCHS), loss.item())\n",
    "torch.save(ffn_model, 'ffn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = BadmintonDataset(testing_data)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "ffn_model = torch.load('ffn.pt')\n",
    "ffn_model.eval()\n",
    "\n",
    "num_corrects = 0\n",
    "num_total = 0\n",
    "predict = []\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        inputs = data.view(-1, 250)\n",
    "        inputs = inputs.to(device).float()\n",
    "        label = label.to(device).float()\n",
    "        \n",
    "        codes, decoded = autoencoder_model(inputs)\n",
    "        output = ffn_model(codes)\n",
    "        \n",
    "        pred = (torch.sigmoid(output) >= 0.5).long()\n",
    "        predict.extend(pred.view(-1).data.cpu().numpy())\n",
    "        \n",
    "        num_corrects += (pred == label).sum().item()\n",
    "        num_total += len(label)\n",
    "\n",
    "print(num_corrects / num_total)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  6.        ,   0.        ,   5.        ,   8.        ,\n",
       "           0.33333333],\n",
       "        [ 10.        ,   0.        ,  -1.        ,   3.        ,\n",
       "           1.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ],\n",
       "        [-10.        , -10.        , -10.        , -10.        ,\n",
       "         -10.        ]]),\n",
       " array([0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
