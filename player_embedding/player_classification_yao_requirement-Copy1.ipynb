{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from pytorch_metric_learning import losses as ml_losses \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gc\n",
    "import csv\n",
    "\n",
    "import json\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, date, time, timezone, timedelta\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import math\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sn\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import Output\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device = torch.cuda.device(2)\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 17\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "random.seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_players = ['Anders ANTONSEN',\n",
    " 'Anthony Sinisuka GINTING',\n",
    " 'CHEN Long',\n",
    " \n",
    " 'CHOU Tien Chen',\n",
    " 'Jonatan CHRISTIE',\n",
    " 'Kento MOMOTA',\n",
    " 'Khosit PHETPRADAB',\n",
    " 'NG Ka Long Angus',\n",
    " \n",
    " 'SHI Yuqi',\n",
    " \n",
    " 'Viktor AXELSEN',\n",
    " 'WANG Tzu Wei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player2cat(player):\n",
    "    p2c = {'Anders ANTONSEN': 0, 'Anthony Sinisuka GINTING': 1, 'CHEN Long': 2,\n",
    "      'CHOU Tien Chen': 3, 'Jonatan CHRISTIE': 4, 'Kento MOMOTA': 5,\n",
    "     'Khosit PHETPRADAB': 6, 'NG Ka Long Angus': 7,\n",
    "     'SHI Yuqi': 8,  'Viktor AXELSEN': 9, 'WANG Tzu Wei': 10}\n",
    "    return p2c[player]\n",
    "\n",
    "def cat2player(cat):\n",
    "    c2p = {0:'ANTONSEN', 1:'GINTING', 2:'Long',  3:'CHOU', 4:'CHRISTIE',\n",
    "           5:'MOMOTA', 6:'PHETPRADAB', 7:'NG',  8:'SHI', \n",
    "           9:'AXELSEN', 10:'WANG'}\n",
    "    return c2p[cat]\n",
    "\n",
    "def generate_labels(rally_data):\n",
    "    # predict player A and B\n",
    "    playerA = rally_data['name_A'].values[0]\n",
    "    playerB = rally_data['name_B'].values[0]\n",
    "\n",
    "    if playerA in target_players and playerB in target_players:\n",
    "        return np.array([player2cat(playerA)]),  np.array([player2cat(playerB)])\n",
    "    elif playerA not in target_players and playerB in target_players:\n",
    "        return None,  np.array([player2cat(playerB)])\n",
    "    elif playerA in target_players and playerB not in target_players:\n",
    "        return np.array([player2cat(playerA)]),  None\n",
    "    elif playerA in target_players and playerB in target_players:\n",
    "        return None,  None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type2cat(shot_type):\n",
    "    t2c = {'發短球': 0, '長球': 1, '推撲球': 2, '殺球': 3, '接殺防守': 4, '平球': 5,\n",
    "           '網前球': 6, '挑球': 7, '切球': 8, '發長球': 9}\n",
    "    return t2c[shot_type]\n",
    "\n",
    "def process_rally(rally_data):\n",
    "    ## process config\n",
    "    mean_x, std_x = 630., 160.\n",
    "    mean_y, std_y = 470., 105.\n",
    "    \n",
    "    drop_cols = ['rally', 'match_id', 'set', 'rally_id', 'ball_round', 'time', 'frame_num', 'db', 'flaw', 'lose_reason', 'win_reason', 'type', 'server', # no need\n",
    "                 'hit_area', 'landing_area', 'player_location_area', 'opponent_location_area', # area dup with x/y\n",
    "                 'name_A', 'name_B', 'getpoint_player', 'roundscore_A', 'roundscore_B', # rally-wise features, maybe use later\n",
    "                 'landing_height', 'landing_x', 'landing_y'] # landing info is dup with hitting\n",
    "    \n",
    "    ## Get player name for checking\n",
    "    playerA = rally_data['name_A'].values[0]\n",
    "    playerB = rally_data['name_B'].values[0]    \n",
    "    \n",
    "    ## process frame_num (time), get frame difference between last shot and this shot, 0 if serve ball \n",
    "    frame_diff = np.pad(rally_data['frame_num'].values[1:] - rally_data['frame_num'].values[:-1], (1, 0), mode='constant')\n",
    "    rally_data['frame_diff'] = frame_diff\n",
    "    \n",
    "    ## NaN convert to binary\n",
    "    rally_data['aroundhead'] = (rally_data['aroundhead'] == 1).astype(int)\n",
    "    rally_data['backhand'] = (rally_data['backhand'] == 1).astype(int)\n",
    "    \n",
    "    ## Player A/B, convert to binary\n",
    "    rally_data['player'] = (rally_data['player'] == 'A').astype(int)\n",
    "    \n",
    "    ## height convert to binary\n",
    "    rally_data['hit_height'] = (rally_data['hit_height'] -1)\n",
    "    rally_data['landing_height'] = (rally_data['landing_height'] -1)\n",
    "    \n",
    "    ## hit_x, hit_y fill with player location\n",
    "    rally_data['hit_x'].values[0] = rally_data['player_location_x'].values[0]\n",
    "    rally_data['hit_y'].values[0] = rally_data['player_location_y'].values[0]\n",
    "    \n",
    "    ## x/y standardization\n",
    "    rally_data['hit_x'] = (rally_data['hit_x'] - mean_x)/std_x\n",
    "    rally_data['hit_y'] = (rally_data['hit_y'] - mean_y)/std_y\n",
    "    rally_data['landing_x'] = (rally_data['landing_x'] - mean_x)/std_x\n",
    "    rally_data['landing_y'] = (rally_data['landing_y'] - mean_y)/std_y\n",
    "    rally_data['player_location_x'] = (rally_data['player_location_x'] - mean_x)/std_x\n",
    "    rally_data['player_location_y'] = (rally_data['player_location_y'] - mean_y)/std_y\n",
    "    rally_data['opponent_location_x'] = (rally_data['opponent_location_x'] - mean_x)/std_x\n",
    "    rally_data['opponent_location_y'] = (rally_data['opponent_location_y'] - mean_y)/std_y\n",
    "    \n",
    "    # type convert to category\n",
    "    rally_data['type_code'] = [type2cat(t) for t in rally_data['type'].values]\n",
    "    \n",
    "    ## drop unneccesary columns\n",
    "    rally_data.drop(columns=drop_cols, inplace=True)\n",
    "    \n",
    "    ## create a copy of the rally but with opposite player \n",
    "    inverse = rally_data.copy()\n",
    "    inverse['player'] = (inverse['player']+1)%2\n",
    "    \n",
    "    if playerA in target_players and playerB in target_players:\n",
    "        return rally_data.values, inverse.values\n",
    "    elif playerA not in target_players and playerB in target_players:\n",
    "        return None, inverse.values\n",
    "    elif playerA in target_players and playerB not in target_players:\n",
    "        return rally_data.values, None\n",
    "    elif playerA in target_players and playerB in target_players:\n",
    "        return None,  None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    seq, label = zip(*data)\n",
    "    seq = list(seq)\n",
    "    label = list(label)\n",
    "    pairs = [(s, l) for s, l in zip(seq, label)]\n",
    "    pairs.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    seq = [s for s, l in pairs]\n",
    "    label = [l for s, l in pairs]\n",
    "    seq_length = [len(sq) for sq in seq]\n",
    "    seq = rnn_utils.pad_sequence(seq, batch_first=True, padding_value=0)\n",
    "    labels = torch.zeros(0, 1)\n",
    "    for l in label:\n",
    "        labels = torch.cat([labels, l], axis=0)\n",
    "    return seq, seq_length, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'aug_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(np_rally):\n",
    "    if np_rally is None:\n",
    "        return False\n",
    "    else:\n",
    "        return np.isnan(np.sum(np_rally))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2575/2575 [00:02<00:00, 1242.45it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "\n",
    "rids = set()\n",
    "# drop flawed rallies, record label distribution\n",
    "for rally in tqdm(data['rally_id'].unique()):\n",
    "    if data.loc[data['rally_id']==rally]['flaw'].any() or len(data.loc[data['rally_id']==rally])<=2 or rally in [578, 596]:\n",
    "        continue\n",
    "    else:\n",
    "        rids.add(rally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(label2rids, test_ratio):\n",
    "    test = random.sample(label2rids, k=round(len(label2rids)*test_ratio))\n",
    "    train = [rid for rid in label2rids if rid not in test]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayerClassificationDataset(Dataset):\n",
    "    def __init__(self, data, rids, split):\n",
    "        self.data = data\n",
    "        self.rids = rids\n",
    "        self.seqs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        pbar = tqdm(rids)\n",
    "        pbar.set_description('Processing %s rally data'%split)\n",
    "        tmp = [process_rally(self.data.loc[self.data['rally_id']==rally].copy()) for rally in pbar]\n",
    "        for seq1, seq2 in tmp:\n",
    "            if seq1 is not None:\n",
    "                self.seqs.append(seq1)\n",
    "            if seq2 is not None:\n",
    "                self.seqs.append(seq2)\n",
    "        \n",
    "        pbar2 = tqdm(rids)\n",
    "        pbar2.set_description('Generating %s labels'%split)\n",
    "        tmp = [generate_labels(self.data.loc[self.data['rally_id']==rally].copy()) for rally in pbar2]\n",
    "        for label1, label2 in tmp:\n",
    "            if label1 is not None:\n",
    "                self.labels.append(label1)\n",
    "            if label2 is not None:\n",
    "                self.labels.append(label2)        \n",
    "        \n",
    "        # checking data are clear, remove those with NaN\n",
    "        self.nan_checking()\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.Tensor(self.seqs[index]), torch.Tensor(self.labels[index]).unsqueeze(0)\n",
    "    \n",
    "    def nan_checking(self):\n",
    "        bad_idxs = [idx for idx in range(len(self.seqs)) if check_nan(self.seqs[idx])]\n",
    "        self.seqs = [seq for idx, seq in enumerate(self.seqs) if idx not in bad_idxs]\n",
    "        self.labels = [label for idx, label in enumerate(self.labels) if idx not in bad_idxs]\n",
    "        print(\"Removed %d rallies with NaN value!\"%len(bad_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsequenceClassificationDataset(Dataset):\n",
    "    def __init__(self, data, rids, split, p, threshold):\n",
    "        self.data = data\n",
    "        self.rids = rids\n",
    "        self.seqs = []\n",
    "        self.labels = []\n",
    "        self.p = p\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        pbar = tqdm(rids)\n",
    "        pbar.set_description('Processing %s rally data'%split)\n",
    "        tmp = [process_rally(self.data.loc[self.data['rally_id']==rally].copy()) for rally in pbar]\n",
    "        for seq1, seq2 in tmp:\n",
    "            if seq1 is not None:\n",
    "                self.seqs.append(seq1)\n",
    "            if seq2 is not None:\n",
    "                self.seqs.append(seq2)\n",
    "        \n",
    "        pbar2 = tqdm(rids)\n",
    "        pbar2.set_description('Generating %s labels'%split)\n",
    "        tmp = [generate_labels(self.data.loc[self.data['rally_id']==rally].copy()) for rally in pbar2]\n",
    "        for label1, label2 in tmp:\n",
    "            if label1 is not None:\n",
    "                self.labels.append(label1)\n",
    "            if label2 is not None:\n",
    "                self.labels.append(label2)        \n",
    "        \n",
    "        # checking data are clear, remove those with NaN\n",
    "        self.nan_checking()\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ### if sequence length is longer than a threshold, random sample a sub-sequence with probability p\n",
    "        dice = random.random()\n",
    "        seq = self.seqs[index]\n",
    "        if dice < self.p and len(seq) >= self.threshold:\n",
    "            sub_len = random.randint(5, len(seq))\n",
    "            start = random.randint(0, len(seq) - sub_len)\n",
    "            sub_seq = seq[start:start+sub_len]\n",
    "            return torch.Tensor(sub_seq), torch.Tensor(self.labels[index]).unsqueeze(0)\n",
    "        else:\n",
    "            return torch.Tensor(seq), torch.Tensor(self.labels[index]).unsqueeze(0)\n",
    "    \n",
    "    def nan_checking(self):\n",
    "        bad_idxs = [idx for idx in range(len(self.seqs)) if check_nan(self.seqs[idx])]\n",
    "        self.seqs = [seq for idx, seq in enumerate(self.seqs) if idx not in bad_idxs]\n",
    "        self.labels = [label for idx, label in enumerate(self.labels) if idx not in bad_idxs]\n",
    "        print(\"Removed %d rallies with NaN value!\"%len(bad_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletSamplingDataset(Dataset):\n",
    "    def __init__(self, dataset, num_classes, weights=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.dataset = dataset\n",
    "        self.num_seqs = len(self.dataset)\n",
    "        if weights is not None:\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            # use uniform if weights not provided\n",
    "            self.weights = np.array([1 for i in range(self.num_classes)])\n",
    "        \n",
    "        self.label2idxs = {i: [] for i in range(num_classes)}\n",
    "        self.build_label2idxs()\n",
    "        \n",
    "        self.dist_mtrx = np.random.rand(self.num_seqs, self.num_seqs)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_seqs\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        anchor, anchor_label = self.dataset[index]\n",
    "        positive, positive_label = self.dataset[self.sample_positive(index, anchor_label.long().item())]\n",
    "        negative, negative_label = self.dataset[self.sample_negative(index, anchor_label.long().item())]\n",
    "        return anchor, positive, negative, anchor_label, positive_label, negative_label\n",
    "    \n",
    "    def build_label2idxs(self):\n",
    "        for idx, (seq, label) in enumerate(self.dataset):\n",
    "            label = label.long().item()\n",
    "            self.label2idxs[label].append(idx)\n",
    "    \n",
    "    def sample_positive(self, anchor_index, label):\n",
    "        pool = self.label2idxs[label].copy()\n",
    "        pool.remove(anchor_index)\n",
    "        return random.choice(pool)\n",
    "    \n",
    "    def sample_negative(self, anchor_index, label):\n",
    "        # First sample the negative class according to given class weight\n",
    "        # Then perform distance weighted sampling on the target class's samples\n",
    "        class_pool = np.array([i for i in range(self.num_classes) if i!=label])\n",
    "        weights = self.weights[class_pool]\n",
    "        sample_class = np.random.choice(class_pool, size=1, p=weights/weights.sum())[0].astype(int)\n",
    "        pool = self.label2idxs[sample_class].copy()\n",
    "        # do sorting to prevent potential problem, might cause efficiency problem though\n",
    "        pool.sort()\n",
    "        # pool is a list of idx, so we now get distance of these idx to anchor\n",
    "        dist = self.dist_mtrx[anchor_index][pool]\n",
    "        # uniformly sample a distance in [dist.min(), dist.max()]\n",
    "        sampled_dist = np.random.uniform(dist.min(), dist.max())\n",
    "        # get the index which have closest distance to the sampled distance\n",
    "        closest_idx = np.argmin(np.power(dist - sampled_dist, 2))\n",
    "        return pool[closest_idx]\n",
    "    \n",
    "    def distance_weighted_negative_sample(self, anchor_index, label):\n",
    "        class_pool = np.array([i for i in range(self.num_classes) if i!=label])\n",
    "        index_pool = []\n",
    "        for label in class_pool:\n",
    "            index_pool.extend(self.label2idxs[label].copy())\n",
    "        index_pool.sort()\n",
    "        dist_list = self.dist_mtrx[anchor_index][index_pool]\n",
    "        dist_list = [dist if dist < 1e5 else 1e5 for dist in dist_list]\n",
    "        dist_list = [dist if dist != 0 else 1e-5 for dist in dist_list]\n",
    "        dist_list = np.array(dist_list)\n",
    "        dist_list = 1 / dist_list\n",
    "        sample_idx = np.random.choice(index_pool, size=1, p=dist_list/dist_list.sum())[0].astype(int)\n",
    "        return sample_idx\n",
    "    \n",
    "    def update_distance(self, embeddings):\n",
    "        self.dist_mtrx = torch.cdist(embeddings, embeddings, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary').cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_collate(data):\n",
    "    anchor, positive, negative, anchor_label, positive_label, negative_label = zip(*data)\n",
    "    anchor, positive, negative = list(anchor), list(positive), list(negative)\n",
    "    anchor_label, positive_label, negative_label = list(anchor_label), list(positive_label), list(negative_label)\n",
    "\n",
    "    anchor_label = torch.tensor(anchor_label)\n",
    "    anchor_len = [len(a) for a in anchor]\n",
    "    anchor = rnn_utils.pad_sequence(anchor, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "    positive_label = torch.tensor(positive_label)\n",
    "    positive_len = [len(p) for p in positive]\n",
    "    positive = rnn_utils.pad_sequence(positive, batch_first=True, padding_value=0)\n",
    "\n",
    "    negative_label = torch.tensor(negative_label)\n",
    "    negative_len = [len(n) for n in negative]\n",
    "    negative = rnn_utils.pad_sequence(negative, batch_first=True, padding_value=0)\n",
    "\n",
    "    return anchor, positive, negative, anchor_len, positive_len, negative_len, anchor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, out_dim, GRU_layers):\n",
    "        super(CNNRNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.GRU_layers = GRU_layers\n",
    "        \n",
    "        self.type_embedding = nn.Embedding(10, self.embed_dim)\n",
    "        self.proj = nn.Linear(self.input_dim - 1, self.hidden_dim - self.embed_dim)\n",
    "        self.fc1 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(self.hidden_dim, self.hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.GRU = nn.GRU(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.GRU_layers, bias=True, batch_first=True, bidirectional=True)\n",
    "        #self.output = MLP(self.hidden_dim, self.hidden_dim//16, self.out_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim*2, self.hidden_dim)\n",
    "        self.output = nn.Linear(self.hidden_dim, self.out_dim) # in_dim is hidden_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, seq, seq_length):\n",
    "        feats = seq[:, :, :-1]\n",
    "        code = seq[:, :, -1].long()\n",
    "        embed = self.type_embedding(code)\n",
    "        feats_proj = self.proj(feats)\n",
    "        x = torch.cat([feats_proj, embed], axis=-1)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         x = self.conv1(x)\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         x = self.relu(x)\n",
    "        \n",
    "        x = rnn_utils.pack_padded_sequence(x, seq_length, batch_first=True, enforce_sorted=False)\n",
    "        output, h_n = self.GRU(x)\n",
    "        # output: [batch_size , seq_len, hidden_dim], h_n: [num_layers, batch_size, hidden_dim]\n",
    "        #x = h_n.permute(1, 0, 2)[:, -1, :]\n",
    "        out_pad, out_len = rnn_utils.pad_packed_sequence(output, batch_first=True)\n",
    "        x = out_pad[torch.arange(out_len.shape[0]), out_len-1, :]\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        y = self.output(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDistanceLoss(nn.Module):\n",
    "    def __init__(self, num_classes, weight=None):\n",
    "        super(EmbeddingDistanceLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, output, label, embeddings):\n",
    "        batch_size = output.shape[0]\n",
    "        dist_mtrx = torch.cdist(output, embeddings, p=2)\n",
    "        dist_mtrx = dist_mtrx*self.weight\n",
    "\n",
    "        return (torch.mean(torch.tensor([-0.1/self.num_classes]).cuda()*dist_mtrx) + torch.mean(self.num_classes*dist_mtrx[:, label]))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_class_embedding(output, label, class_embedding, alpha):\n",
    "    tmp_embedding = torch.clone(class_embedding)\n",
    "    tmp_embedding[label, :] = class_embedding[label, :]*(1-alpha) + output*alpha\n",
    "    class_embedding = tmp_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrices(preds, labels, num_classes):\n",
    "    acc = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    count = [] \n",
    "    \n",
    "    TP_all = 0\n",
    "    FP_all = 0\n",
    "    TN_all = 0\n",
    "    FN_all = 0\n",
    "    for target in range(num_classes):\n",
    "        cnt = 0\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "        for i in range(len(preds)):\n",
    "            if labels[i]==target:\n",
    "                cnt += 1\n",
    "            if preds[i]==target and labels[i]==target:\n",
    "                TP += 1\n",
    "                TP_all += 1\n",
    "            elif preds[i]==target and labels[i]!=target:\n",
    "                FP += 1\n",
    "                FP_all += 1\n",
    "            elif preds[i]!=target and labels[i]!=target:\n",
    "                TN += 1\n",
    "                TN_all += 1\n",
    "            elif preds[i]!=target and labels[i]==target:\n",
    "                FN += 1\n",
    "                FN_all += 1\n",
    "        acc.append((TP+TN)/(TP+FP+TN+FN+1e-10))\n",
    "        precision.append((TP)/(TP+FP+1e-10))\n",
    "        recall.append(TP/(TP+FN+1e-10))\n",
    "        f1.append(2*(precision[target]*recall[target])/(precision[target]+recall[target]+1e-10))\n",
    "        count.append(cnt)\n",
    "            \n",
    "    num_exist_class = len([1 for c in count if c!=0]) + 1e-10\n",
    "    return sum(acc)/num_exist_class, sum(precision)/num_exist_class, sum(recall)/num_exist_class, sum(f1)/num_exist_class, None\n",
    "\n",
    "def confusion_matrix(preds, labels, num_classes, ax):\n",
    "    conf = np.zeros((num_classes, num_classes))\n",
    "    for i in range(len(preds)):\n",
    "        conf[preds[i]][labels[i]] += 1\n",
    "\n",
    "    norm_vec = np.sum(conf, axis=0)\n",
    "    conf = np.around(conf/norm_vec, decimals=2)\n",
    "    df_cm = pd.DataFrame(conf, [cat2player(i) for i in range(num_classes)], [cat2player(i) for i in range(num_classes)])\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 12}, cmap='GnBu', fmt='g', ax=ax)\n",
    "    b, t = ax.get_ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    ax.set_title(\"Confusion Matrix of Player Classification\")\n",
    "    ax.set_ylim(b, t) # update the ylim(bottom, top) values\n",
    "    ax.xaxis.set_tick_params(rotation=45)\n",
    "    ax.yaxis.set_tick_params(rotation=45)\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Prediction')\n",
    "    for item in ([ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12)\n",
    "    ax.title.set_fontsize(16)\n",
    "\n",
    "\n",
    "def validation(net, class_embedding, testloader, num_classes, epoch, ax, conf=False):\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for idx, (seq, seq_length, label) in enumerate(testloader):\n",
    "        # seq: [batch x padded length x feat dim]\n",
    "        # label [batch x 2]\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        seq, label = torch.Tensor(seq).cuda(), torch.Tensor(label).long().cuda()\n",
    "        output,_ = net(seq, seq_length)\n",
    "        \n",
    "        pred = torch.argmin(torch.cdist(output, class_embedding, p=2), dim=-1)\n",
    "        #pred = torch.argmax(output, dim=-1)\n",
    "        \n",
    "        labels.append(label.cpu().item())\n",
    "        preds.append(pred.detach().cpu().item())\n",
    "\n",
    "    acc, prec, rec, f1, ACC = metrices(preds, labels, num_classes)\n",
    "    if conf:\n",
    "        confusion_matrix(preds, labels, num_classes, ax)\n",
    "    #print(\"Epoch No.%d\\tAcc: %3f, F1: %3f\"%(epoch, rec, f1))\n",
    "    return rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(lst, wind_size):\n",
    "    lst = [sum(lst[i:i+wind_size])/wind_size for i in range(0, len(lst)-wind_size)]\n",
    "    return lst\n",
    "\n",
    "def plots(losses, accs, f1s, wind_size, ax1):\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    y1 = moving_average(losses, wind_size)\n",
    "    x1 = [i for i in range(len(y1))]\n",
    "    \n",
    "    y2 = accs\n",
    "    x2_step = round(len(x1)/len(y2))\n",
    "    x2 = [i*x2_step for i in range(1, len(y2)+1)]\n",
    "    \n",
    "    y3 = f1s\n",
    "\n",
    "    curve1, = ax1.plot(x1, y1, label=\"Training loss\", color='r')\n",
    "    curve2, = ax2.plot(x2, y2, label=\"Test accuracy\", color='b')\n",
    "    curve3, = ax2.plot(x2, y3, label=\"Test F1\", color='g')\n",
    "    \n",
    "    curves = [curve1, curve2, curve3]\n",
    "    ax1.legend(curves, [curve.get_label() for curve in curves], loc='center right')\n",
    "    ax1.set_xlabel(\"Iteration\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax2.set_ylabel(\"Acc/F1\")\n",
    "    \n",
    "    ax1.set_title(\"Learning Curve\")\n",
    "    \n",
    "    for item in ([ax1.xaxis.label, ax1.yaxis.label, ax2.yaxis.label] + ax1.get_xticklabels() + ax1.get_yticklabels() +  ax2.get_yticklabels()):\n",
    "        item.set_fontsize(12)\n",
    "    ax1.title.set_fontsize(16)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_embedding(num_classes, hidden_dim, model, pc_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        class_embedding = torch.zeros(num_classes, hidden_dim).cuda()\n",
    "        class_count = torch.zeros(num_classes).cuda()\n",
    "\n",
    "        for seq, seq_length, labels in pc_loader:\n",
    "            seq = seq.cuda()\n",
    "\n",
    "            # labels: [batch_size]\n",
    "            labels = labels.squeeze(1).long().cuda()\n",
    "            # output: [batch_size x hidden_dim*2]\n",
    "            output,_ = model(seq, seq_length)\n",
    "\n",
    "            # add embeddings to corresponding class_embedding\n",
    "            #scatter [num_classes x batch_size]\n",
    "            scatter = torch.zeros(num_classes, labels.shape[0]).cuda()\n",
    "            scatter[labels, torch.tensor([i for i in range(labels.shape[0])])] = 1\n",
    "\n",
    "            # add class count\n",
    "            class_count += scatter.sum(axis=1)\n",
    "\n",
    "            # scatter*output [num_classes x hidden_dim*2]\n",
    "            class_embedding += torch.matmul(scatter, output)\n",
    "\n",
    "        class_embedding /= class_count.unsqueeze(1).expand(num_classes, hidden_dim)\n",
    "    \n",
    "    return class_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_embedding(num_classes, hidden_dim, model, pc_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        embeddings = torch.zeros(0, hidden_dim).cuda()\n",
    "        classes = torch.zeros(0).long().cuda()\n",
    "\n",
    "        for seq, seq_length, labels in pc_loader:\n",
    "            seq = seq.cuda()\n",
    "\n",
    "            # labels: [batch_size]\n",
    "            labels = labels.squeeze(1).long().cuda()\n",
    "            # output: [batch_size x hidden_dim*2]\n",
    "            output,_ = model(seq, seq_length)\n",
    "\n",
    "            embeddings = torch.cat([embeddings, output], axis=0)\n",
    "            classes = torch.cat([classes, labels], axis=0)\n",
    "            \n",
    "    return embeddings, classes.cpu().numpy()\n",
    "    \n",
    "def tsne_viz_train_test(train_embeddings, train_labels, test_embeddings, test_labels, class_embedding, ax, mode):\n",
    "    train_range = train_embeddings.shape[0]\n",
    "    test_range = train_range + test_embeddings.shape[0]\n",
    "    embeddings = np.concatenate((train_embeddings, test_embeddings, class_embedding), axis=0)\n",
    "    labels = np.concatenate((train_labels, test_labels, np.array([i for i in range(class_embedding.shape[0])])), axis=0)\n",
    "\n",
    "    if mode=='tsne':\n",
    "        reduced = TSNE(n_components=2, perplexity=30.0).fit_transform(embeddings)\n",
    "    elif mode=='pca':\n",
    "        reduced = PCA(n_components=2, svd_solver='full').fit_transform(embeddings)\n",
    "    else:\n",
    "        raise NotImplemented('Only t-SNE and PCA visualizations are supported currenly')\n",
    "    \n",
    "    train_x = reduced[:train_range, 0]\n",
    "    train_y = reduced[:train_range, 1]\n",
    "    train_group = labels[:train_range]\n",
    "    \n",
    "    test_x = reduced[train_range:test_range, 0]\n",
    "    test_y = reduced[train_range:test_range, 1]\n",
    "    test_group = labels[train_range:test_range]\n",
    "    \n",
    "    class_x = reduced[test_range:, 0]\n",
    "    class_y = reduced[test_range:, 1]\n",
    "    class_group = labels[test_range:]\n",
    "\n",
    "    \n",
    "    cdict = {0:'black', 1:'silver', 2:'lightcoral', 3:'red', 4:'sienna', 5:'orange',\n",
    "           6:'yellow', 7:'green', 8:'lime', 9:'blue', 10:'cyan', 11:'purple',\n",
    "           12:'lightblue', 13:'deeppink', 14:'darkmagenta', 15:'dodgerblue', 16:'lime'}\n",
    "\n",
    "    for g in np.unique(train_group):\n",
    "        ix = np.where(train_group == g)\n",
    "        ax.scatter(train_x[ix], train_y[ix], c = cdict[g], s = 4, alpha=0.05)\n",
    "    \n",
    "    for g in np.unique(test_group):\n",
    "        ix = np.where(test_group == g)\n",
    "        ax.scatter(test_x[ix], test_y[ix], c = cdict[g], label = cat2player(g), s = 10, alpha=0.8)\n",
    "    \n",
    "    for g in np.unique(class_group):\n",
    "        ix = np.where(class_group == g)\n",
    "        ax.scatter(class_x[ix], class_y[ix], c = cdict[g], s = 200, marker = '*', edgecolors='black')\n",
    "    \n",
    "    ax.legend()\n",
    "    if mode == 'tsne':\n",
    "        ax.set_title('Visualization of Embeddings (t-SNE)')\n",
    "    else:\n",
    "        ax.set_title('Visualization of Embeddings (PCA)')\n",
    "    ax.axes.xaxis.set_ticks([])\n",
    "    ax.axes.yaxis.set_ticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train rally data: 100%|██████████| 1948/1948 [00:11<00:00, 176.56it/s]\n",
      "Generating train labels: 100%|██████████| 1948/1948 [00:00<00:00, 2004.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 rallies with NaN value!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test rally data: 100%|██████████| 487/487 [00:02<00:00, 177.63it/s]\n",
      "Generating test labels: 100%|██████████| 487/487 [00:00<00:00, 2076.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 rallies with NaN value!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(rids, 0.2)\n",
    "\n",
    "pc_dataset = PlayerClassificationDataset(data, train, 'train')\n",
    "#pc_dataset = SubsequenceClassificationDataset(data, train, 'train', p=0.5, threshold=5)\n",
    "weights = np.array([1, 1., 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "train_dataset = TripletSamplingDataset(pc_dataset, num_classes=11, weights=weights)\n",
    "test_dataset = PlayerClassificationDataset(data, test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_loader = DataLoader(pc_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=triplet_collate)\n",
    "testloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "num_classes = 11\n",
    "hidden_dim = 2048\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = SimpleNet(12, 128, hidden_dim, num_classes, 2).cuda()\n",
    "net = CNNRNN(12, 128, hidden_dim, num_classes, 2).cuda()\n",
    "class_embedding = torch.randn(num_classes, hidden_dim).cuda()\n",
    "class_embedding.require_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "triplet_loss = nn.TripletMarginLoss(margin=0.5, p=2).cuda()\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings, _ = generate_all_embedding(num_classes, hidden_dim, net, pc_loader)\n",
    "class_embedding = generate_class_embedding(num_classes, hidden_dim, net, pc_loader)\n",
    "\n",
    "trainloader.dataset.update_distance(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5292629e1cf8420c8dd50571d57ee6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/500 [13:48<11:26:14, 84.03s/it]"
     ]
    }
   ],
   "source": [
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "losses = []\n",
    "accs = [0]\n",
    "f1s = [0]\n",
    "gc.collect()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    net.train()\n",
    "    for idx, (anchor, positive, negative, anchor_len, positive_len, negative_len, anchor_label) in enumerate(trainloader):\n",
    "        anchor, positive, negative = anchor.cuda(), positive.cuda(), negative.cuda()\n",
    "        anchor_label = anchor_label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        anchor_embedding, acnhor_logits = net(anchor, anchor_len)\n",
    "        positive_embedding, _ = net(positive, positive_len)\n",
    "        negative_embedding, _ = net(negative, negative_len)\n",
    "      \n",
    "        t_loss = triplet_loss(anchor_embedding, positive_embedding, negative_embedding)\n",
    "\n",
    "        loss = t_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        losses.append((loss).item())\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 25 == 0:\n",
    "        train_embeddings, train_labels = generate_all_embedding(num_classes, hidden_dim, net, pc_loader)\n",
    "        # update dist mtrx\n",
    "        trainloader.dataset.update_distance(train_embeddings)\n",
    "        test_embeddings, test_labels = generate_all_embedding(num_classes, hidden_dim, net, testloader)\n",
    "        class_embedding = generate_class_embedding(num_classes, hidden_dim, net, pc_loader)\n",
    "        with out:\n",
    "            #clear_output(wait=True)\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(25, 10))\n",
    "            acc, f1 = validation(net, class_embedding, testloader, num_classes, epoch, axes[0], conf=True)\n",
    "            accs.append(acc)\n",
    "            f1s.append(f1)\n",
    "            plots(losses, accs, f1s, 10, axes[1])\n",
    "            plt.show()\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(25, 10))\n",
    "            \n",
    "            tsne_viz_train_test(train_embeddings.cpu().numpy(), train_labels, test_embeddings.cpu().numpy(), test_labels, class_embedding.cpu(), axes[0], 'pca')\n",
    "            tsne_viz_train_test(train_embeddings.cpu().numpy(), train_labels, test_embeddings.cpu().numpy(), test_labels, class_embedding.cpu(), axes[1], 'tsne')\n",
    "            plt.show()\n",
    "\n",
    "    else:\n",
    "        train_embeddings, train_labels = generate_all_embedding(num_classes, hidden_dim, net, pc_loader)\n",
    "        # update dist mtrx every epoch\n",
    "        trainloader.dataset.update_distance(train_embeddings)\n",
    "        class_embedding = generate_class_embedding(num_classes, hidden_dim, net, pc_loader)\n",
    "        acc, f1 = validation(net, class_embedding, testloader, num_classes, epoch, ax=None)\n",
    "        accs.append(acc)\n",
    "        f1s.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'for_5_29_no_girl_re.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_distance_viz(class_embedding):\n",
    "    class_dist = torch.cdist(class_embedding, class_embedding, p=2).cpu().numpy()#/class_embedding.shape[1]\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    df = pd.DataFrame(class_dist, [cat2player(i) for i in range(num_classes)], [cat2player(i) for i in range(num_classes)])\n",
    "    \n",
    "    sn.heatmap(df, annot=True, annot_kws={\"size\": 10}, cmap='GnBu', fmt='.2f', ax=ax, square=True)\n",
    "    b, t = ax.get_ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    ax.set_title(\"Distance of Embedding between Players\")\n",
    "    ax.set_ylim(b, t) # update the ylim(bottom, top) values\n",
    "    ax.xaxis.set_tick_params(rotation=90)\n",
    "    ax.yaxis.set_tick_params(rotation=0)\n",
    "    ax.set_xlabel('Player')\n",
    "    ax.set_ylabel('Player')\n",
    "    for item in ([ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12)\n",
    "    ax.title.set_fontsize(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distance_viz(class_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = class_embedding.clone().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2c = {'Anders ANTONSEN': 0, 'Anthony Sinisuka GINTING': 1, 'CHEN Long': 2,\n",
    "      'CHOU Tien Chen': 3, 'Jonatan CHRISTIE': 4, 'Kento MOMOTA': 5,\n",
    "     'Khosit PHETPRADAB': 6, 'NG Ka Long Angus': 7,\n",
    "     'SHI Yuqi': 8,  'Viktor AXELSEN': 9, 'WANG Tzu Wei': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dict(zip(list(p2c.keys()), embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('5_29_player_embedding_no_girl_re', 'wb') as handle:\n",
    "    pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5/29"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
